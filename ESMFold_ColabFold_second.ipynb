{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyManGPO7QTIaumBAR58+7TE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/espickle1/esmfold_colabfold/blob/main/ESMFold_ColabFold_second.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpxAvRtxZxrX"
      },
      "outputs": [],
      "source": [
        "### Colabfold running ESMFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Install necessary packages\n",
        "%%time\n",
        "version = \"1\"\n",
        "model_name = \"esmfold_v0.model\" if version == \"0\" else \"esmfold.model\"\n",
        "\n",
        "import os, time\n",
        "\n",
        "if not os.path.isfile(model_name):\n",
        "  # download esmfold params\n",
        "  os.system(\"apt-get install aria2 -qq\")\n",
        "  os.system(f\"aria2c -q -x 16 https://colabfold.steineggerlab.workers.dev/esm/{model_name} &\")\n",
        "\n",
        "  if not os.path.isfile(\"finished_install\"):\n",
        "    # install libs\n",
        "    print(\"installing libs...\")\n",
        "    os.system(\"pip install -q omegaconf pytorch_lightning biopython ml_collections einops py3Dmol modelcif\")\n",
        "    os.system(\"pip install -q git+https://github.com/NVIDIA/dllogger.git\")\n",
        "\n",
        "    print(\"installing openfold...\")\n",
        "    # install openfold\n",
        "    os.system(f\"pip install -q git+https://github.com/sokrypton/openfold.git\")\n",
        "\n",
        "    print(\"installing esmfold...\")\n",
        "    # install esmfold\n",
        "    os.system(f\"pip install -q git+https://github.com/sokrypton/esm.git\")\n",
        "    os.system(\"touch finished_install\")\n",
        "\n",
        "  # wait for Params to finish downloading...\n",
        "  while not os.path.isfile(model_name):\n",
        "    time.sleep(5)\n",
        "  if os.path.isfile(f\"{model_name}.aria2\"):\n",
        "    print(\"downloading params...\")\n",
        "  while os.path.isfile(f\"{model_name}.aria2\"):\n",
        "    time.sleep(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jthGQKfGZ1ww",
        "outputId": "962fa7a9-f0a7-4dd3-e957-f3051250bf2b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing libs...\n",
            "installing openfold...\n",
            "installing esmfold...\n",
            "CPU times: user 581 ms, sys: 118 ms, total: 699 ms\n",
            "Wall time: 3min 55s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import dependencies\n",
        "import torch\n",
        "from jax.tree_util import tree_map\n",
        "import gc\n",
        "\n",
        "from string import ascii_uppercase, ascii_lowercase\n",
        "import hashlib, re, os\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "import py3Dmol\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7cyvhSZZ3iZ",
        "outputId": "5cded643-82fe-4fa5-8518-2ef388dcfd46"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Parse quantitative data for output\n",
        "def parse_output(output):\n",
        "  # Parse PAE and pLDDT\n",
        "  pae = (output[\"aligned_confidence_probs\"][0] * np.arange(64)).mean(-1) * 31\n",
        "  plddt = output[\"plddt\"][0,:,1]\n",
        "\n",
        "  # Parse contacts\n",
        "  bins = np.append(0,np.linspace(2.3125,21.6875,63))\n",
        "  sm_contacts = softmax(output[\"distogram_logits\"],-1)[0]\n",
        "  sm_contacts = sm_contacts[...,bins<8].sum(-1)\n",
        "\n",
        "  # Create overall position matrix\n",
        "  xyz = output[\"positions\"][-1,0,:,1]\n",
        "  mask = output[\"atom37_atom_exists\"][0,:,1] == 1\n",
        "\n",
        "  # Combine output data\n",
        "  o = {\"pae\":pae[mask,:][:,mask],\n",
        "       \"plddt\":plddt[mask],\n",
        "       \"sm_contacts\":sm_contacts[mask,:][:,mask],\n",
        "       \"xyz\":xyz[mask]}\n",
        "\n",
        "  return o\n",
        "\n",
        "## Get has for unique item ID\n",
        "def get_hash(x): return hashlib.sha1(x.encode()).hexdigest()"
      ],
      "metadata": {
        "id": "cJtnYl8SZ7px"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load model\n",
        "if \"model\" not in dir() or model_name != model_name_:\n",
        "  if \"model\" in dir():\n",
        "    # delete old model from memory\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  # Load the model directly\n",
        "  model = torch.load(model_name)\n",
        "  model.eval().cuda().requires_grad_(False)\n",
        "  model_name_ = model_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPsKQP0AaCAH",
        "outputId": "1fb6b18c-9eee-4efc-ebb7-8a75ab01564e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-7ded6d56cced>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(model_name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import as a function\n",
        "def sequence_read(sequence_input, position, copies):\n",
        "  # Read translated sequences and parse to clean up invalid charaacters\n",
        "  sequence = sequence_input.loc[position]['Translation']\n",
        "  sequence_clean = re.sub(\"[^A-Z:]\", \"\", sequence.replace(\"/\",\":\").upper())\n",
        "  sequence_clean = re.sub(\":+\",\":\",sequence)\n",
        "  sequence_clean = re.sub(\"^[:]+\",\"\",sequence)\n",
        "  sequence_clean = re.sub(\"[:]+$\",\"\",sequence)\n",
        "\n",
        "  # Read meta data from the file\n",
        "  meta = sequence_input.loc[position]['meta']\n",
        "\n",
        "  # Join multimers into one sequence\n",
        "  if copies == \"\" or copies <= 0: copies = 1\n",
        "  sequence = \":\".join([sequence] * copies)\n",
        "\n",
        "  # Come up with sequence ID and overall predicted length\n",
        "  ID = \"No_\" + str(position) + \"_\" + jobname+\"_\"+get_hash(sequence)[:5]\n",
        "  seqs = sequence.split(\":\")\n",
        "  lengths = [len(s) for s in seqs]\n",
        "  length = sum(lengths)\n",
        "  print(\"length\",length)\n",
        "\n",
        "  # Determine the type of multimer\n",
        "  u_seqs = list(set(seqs))\n",
        "  if len(seqs) == 1: mode = \"mono\"\n",
        "  elif len(u_seqs) == 1: mode = \"homo\"\n",
        "  else: mode = \"hetero\"\n",
        "\n",
        "  return sequence_clean, meta, copies, ID, u_seqs, length, lengths"
      ],
      "metadata": {
        "id": "F-o5Z2O7aDuw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Predict protein structure from input sequence\n",
        "def prediction_block(sequence, ID, row_number):\n",
        "  # Determine chunk size for speedup\n",
        "  # Following scheme is used (700 cutoff)\n",
        "  # A100: 312 for smaller, 192 for larger\n",
        "  # L4: 256 for smaller, 164 for larger\n",
        "  # T4: 128 for smaller, 64 for larger\n",
        "  length = len(sequence)\n",
        "  if length > 700:\n",
        "    model.set_chunk_size(192)\n",
        "  else:\n",
        "    model.set_chunk_size(312)\n",
        "\n",
        "  # Empty cuda cache\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Inference\n",
        "  output = model.infer(\n",
        "      sequence,\n",
        "      num_recycles=num_recycles,\n",
        "      chain_linker=\"X\"*chain_linker,\n",
        "      residue_index_offset=512\n",
        "      )\n",
        "\n",
        "  # Parse outputs into exportable format\n",
        "  pdb_str = model.output_to_pdb(output)[0]\n",
        "  output = tree_map(lambda x: x.cpu().numpy(), output)\n",
        "  ptm = output[\"ptm\"][0]\n",
        "  plddt = output[\"plddt\"][0,...,1].mean()\n",
        "  O = parse_output(output)\n",
        "  print(f'ptm: {ptm:.3f} plddt: {plddt:.3f}')\n",
        "\n",
        "  # Save results into files\n",
        "  prefix = f\"{ID}_ptm{ptm:.3f}_r{num_recycles}_default\"\n",
        "  np.savetxt(f\"{prefix}.pae.txt\",O[\"pae\"],\"%.3f\")\n",
        "  with open(f\"{prefix}.pdb\",\"w\") as out:\n",
        "    out.write(pdb_str)\n",
        "\n",
        "  # return pdb_str, prefix, O\n",
        "  return"
      ],
      "metadata": {
        "id": "YcThVBCJaFHh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import settings: manual settings\n",
        "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
        "\n",
        "num_recycles = 3\n",
        "chain_linker = 25\n",
        "multimer_n = 1\n",
        "# row_number = 12\n",
        "\n",
        "jobname = \"dir_test\"\n",
        "jobname = re.sub(r'\\W+', '', jobname)[:50]\n",
        "\n",
        "input_directory = \"/content/drive/MyDrive/ww_virome/esmfold_colab/sequences/\"\n",
        "output_directory = \"/content/drive/MyDrive/ww_virome/esmfold_colab/structures\"\n",
        "os.chdir(output_directory)\n",
        "file_path = f\"{input_directory}big_merge_norovirus_translation_20.csv\"\n",
        "sequence_file = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "nQZ7NeD4aG6Q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''sequence_clean, meta, copies, ID, u_seqs, length, lengths = sequence_read(\n",
        "    sequence_file,\n",
        "    row_number,\n",
        "    copies=multimer_n\n",
        "    )\n",
        "\n",
        "print(f\"Entry {row_number}: {meta}\")\n",
        "\n",
        "# pdb_str, prefix, O = prediction_block(sequence_clean, ID, row_number)\n",
        "start_time = time.time()\n",
        "prediction_block(sequence_clean, ID, row_number)\n",
        "end_time = time.time()\n",
        "print(f\"Inference time: {end_time - start_time:.2f}s\")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf_QIjAMaMdp",
        "outputId": "6c2701fc-66dd-47f4-cf31-d9297550a747"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length 1088\n",
            "ptm: 0.441 plddt: 47.891\n",
            "Inference time for entry 8, AGS43817.1_Equine rotavirus A_RNA-dependent RNA polymerase: 202.88s\n",
            "CPU times: user 3min 20s, sys: 3.61 s, total: 3min 24s\n",
            "Wall time: 3min 23s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get inference for files with multiple entries\n",
        "\n",
        "for _, row in sequence_file.iterrows():\n",
        "  sequence_clean, meta, copies, ID, u_seqs, length, lengths = sequence_read(\n",
        "      sequence_file,\n",
        "      row,\n",
        "      copies=multimer_n\n",
        "      )\n",
        "\n",
        "  print(f\"Entry {row}: {meta}\")\n",
        "\n",
        "  start_time = time.time()\n",
        "  prediction_block(sequence_clean, ID, row)\n",
        "  end_time = time.time()\n",
        "  print(f\"Inference time: {end_time - start_time:.2f}s\")\n"
      ],
      "metadata": {
        "id": "niCuUjyBjCJf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}