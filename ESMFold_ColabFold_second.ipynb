{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMvH8YU7tUxgLciXUi6ejsf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/espickle1/esmfold_colabfold/blob/main/ESMFold_ColabFold_second.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpxAvRtxZxrX"
      },
      "outputs": [],
      "source": [
        "### Colabfold running ESMFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Install necessary packages\n",
        "%%time\n",
        "version = \"1\"\n",
        "model_name = \"esmfold_v0.model\" if version == \"0\" else \"esmfold.model\"\n",
        "\n",
        "import os, time\n",
        "\n",
        "if not os.path.isfile(model_name):\n",
        "  # download esmfold params\n",
        "  os.system(\"apt-get install aria2 -qq\")\n",
        "  os.system(f\"aria2c -q -x 16 https://colabfold.steineggerlab.workers.dev/esm/{model_name} &\")\n",
        "\n",
        "  if not os.path.isfile(\"finished_install\"):\n",
        "    # install libs\n",
        "    print(\"installing libs...\")\n",
        "    os.system(\"pip install -q omegaconf pytorch_lightning biopython ml_collections einops py3Dmol modelcif\")\n",
        "    os.system(\"pip install -q git+https://github.com/NVIDIA/dllogger.git\")\n",
        "\n",
        "    print(\"installing openfold...\")\n",
        "    # install openfold\n",
        "    os.system(f\"pip install -q git+https://github.com/sokrypton/openfold.git\")\n",
        "\n",
        "    print(\"installing esmfold...\")\n",
        "    # install esmfold\n",
        "    os.system(f\"pip install -q git+https://github.com/sokrypton/esm.git\")\n",
        "    os.system(\"touch finished_install\")\n",
        "\n",
        "  # wait for Params to finish downloading...\n",
        "  while not os.path.isfile(model_name):\n",
        "    time.sleep(5)\n",
        "  if os.path.isfile(f\"{model_name}.aria2\"):\n",
        "    print(\"downloading params...\")\n",
        "  while os.path.isfile(f\"{model_name}.aria2\"):\n",
        "    time.sleep(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jthGQKfGZ1ww",
        "outputId": "1bea2fa1-4453-43a7-c878-aa5391c95934"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 32 µs, sys: 0 ns, total: 32 µs\n",
            "Wall time: 35.5 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import dependencies\n",
        "import torch\n",
        "from jax.tree_util import tree_map\n",
        "import gc\n",
        "\n",
        "from string import ascii_uppercase, ascii_lowercase\n",
        "import hashlib, re, os\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "import py3Dmol\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7cyvhSZZ3iZ",
        "outputId": "5af73bb3-4782-463d-c8f3-be35cbca1d55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Parse quantitative data for output\n",
        "def parse_output(output):\n",
        "  # Parse PAE and pLDDT\n",
        "  pae = (output[\"aligned_confidence_probs\"][0] * np.arange(64)).mean(-1) * 31\n",
        "  plddt = output[\"plddt\"][0,:,1]\n",
        "\n",
        "  # Parse contacts\n",
        "  bins = np.append(0,np.linspace(2.3125,21.6875,63))\n",
        "  sm_contacts = softmax(output[\"distogram_logits\"],-1)[0]\n",
        "  sm_contacts = sm_contacts[...,bins<8].sum(-1)\n",
        "\n",
        "  # Create overall position matrix\n",
        "  xyz = output[\"positions\"][-1,0,:,1]\n",
        "  mask = output[\"atom37_atom_exists\"][0,:,1] == 1\n",
        "\n",
        "  # Combine output data\n",
        "  o = {\"pae\":pae[mask,:][:,mask],\n",
        "       \"plddt\":plddt[mask],\n",
        "       \"sm_contacts\":sm_contacts[mask,:][:,mask],\n",
        "       \"xyz\":xyz[mask]}\n",
        "\n",
        "  return o\n",
        "\n",
        "## Get has for unique item ID\n",
        "def get_hash(x): return hashlib.sha1(x.encode()).hexdigest()"
      ],
      "metadata": {
        "id": "cJtnYl8SZ7px"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load model\n",
        "if \"model\" not in dir() or model_name != model_name_:\n",
        "  if \"model\" in dir():\n",
        "    # delete old model from memory\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  # Load the model directly\n",
        "  model = torch.load(model_name)\n",
        "  model.eval().cuda().requires_grad_(False)\n",
        "  model_name_ = model_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPsKQP0AaCAH",
        "outputId": "d193854d-86ef-47e1-fe2e-6b9e627f9973"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-12623ac4d558>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(model_name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import as a function\n",
        "def sequence_read(sequence_input, position, copies):\n",
        "  # Read translated sequences and parse to clean up invalid charaacters\n",
        "  sequence = sequence_input.loc[position]['Translation']\n",
        "  sequence_clean = re.sub(\"[^A-Z:]\", \"\", sequence.replace(\"/\",\":\").upper())\n",
        "  sequence_clean = re.sub(\":+\",\":\",sequence)\n",
        "  sequence_clean = re.sub(\"^[:]+\",\"\",sequence)\n",
        "  sequence_clean = re.sub(\"[:]+$\",\"\",sequence)\n",
        "\n",
        "  # Read meta data from the file\n",
        "  meta = sequence_input.loc[position]['meta']\n",
        "\n",
        "  # Join multimers into one sequence\n",
        "  if copies == \"\" or copies <= 0: copies = 1\n",
        "  sequence = \":\".join([sequence] * copies)\n",
        "\n",
        "  # Come up with sequence ID and overall predicted length\n",
        "  ID = \"No_\" + str(position) + \"_\" + jobname+\"_\"+get_hash(sequence)[:5]\n",
        "  seqs = sequence.split(\":\")\n",
        "  lengths = [len(s) for s in seqs]\n",
        "  length = sum(lengths)\n",
        "  print(\"length\",length)\n",
        "\n",
        "  # Determine the type of multimer\n",
        "  u_seqs = list(set(seqs))\n",
        "  if len(seqs) == 1: mode = \"mono\"\n",
        "  elif len(u_seqs) == 1: mode = \"homo\"\n",
        "  else: mode = \"hetero\"\n",
        "\n",
        "  return sequence_clean, meta, copies, ID, u_seqs, length, lengths"
      ],
      "metadata": {
        "id": "F-o5Z2O7aDuw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Predict protein structure from input sequence\n",
        "def prediction_block(sequence, ID, row_number):\n",
        "  # Determine chunk size for speedup\n",
        "  # Following scheme is used (700 cutoff)\n",
        "  # A100: 312 for smaller, 192 for larger\n",
        "  # L4: 256 for smaller, 164 for larger\n",
        "  # T4: 128 for smaller, 64 for larger\n",
        "  length = len(sequence)\n",
        "  if length > 700:\n",
        "    model.set_chunk_size(192)\n",
        "  else:\n",
        "    model.set_chunk_size(312)\n",
        "\n",
        "  # Empty cuda cache\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Inference\n",
        "  output = model.infer(\n",
        "      sequence,\n",
        "      num_recycles=num_recycles,\n",
        "      chain_linker=\"X\"*chain_linker,\n",
        "      residue_index_offset=512\n",
        "      )\n",
        "\n",
        "  # Parse outputs into exportable format\n",
        "  pdb_str = model.output_to_pdb(output)[0]\n",
        "  output = tree_map(lambda x: x.cpu().numpy(), output)\n",
        "  ptm = output[\"ptm\"][0]\n",
        "  plddt = output[\"plddt\"][0,...,1].mean()\n",
        "  O = parse_output(output)\n",
        "  print(f'ptm: {ptm:.3f} plddt: {plddt:.3f}')\n",
        "\n",
        "  # Save results into files\n",
        "  prefix = f\"{ID}_ptm{ptm:.3f}_r{num_recycles}_default\"\n",
        "  np.savetxt(f\"{prefix}.pae.txt\",O[\"pae\"],\"%.3f\")\n",
        "  with open(f\"{prefix}.pdb\",\"w\") as out:\n",
        "    out.write(pdb_str)\n",
        "\n",
        "  # return pdb_str, prefix, O\n",
        "  return"
      ],
      "metadata": {
        "id": "YcThVBCJaFHh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import settings: manual settings\n",
        "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
        "\n",
        "num_recycles = 3\n",
        "chain_linker = 25\n",
        "multimer_n = 1\n",
        "# row_number = 12\n",
        "\n",
        "jobname = \"dir_test\"\n",
        "jobname = re.sub(r'\\W+', '', jobname)[:50]\n",
        "\n",
        "input_directory = \"/content/drive/MyDrive/ww_virome/esmfold_colab/sequences/\"\n",
        "output_directory = \"/content/drive/MyDrive/ww_virome/esmfold_colab/structures\"\n",
        "os.chdir(output_directory)\n",
        "file_path = f\"{input_directory}big_merge_norovirus_translation_20.csv\"\n",
        "sequence_file = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "nQZ7NeD4aG6Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''sequence_clean, meta, copies, ID, u_seqs, length, lengths = sequence_read(\n",
        "    sequence_file,\n",
        "    row_number,\n",
        "    copies=multimer_n\n",
        "    )\n",
        "\n",
        "print(f\"Entry {row_number}: {meta}\")\n",
        "\n",
        "# pdb_str, prefix, O = prediction_block(sequence_clean, ID, row_number)\n",
        "start_time = time.time()\n",
        "prediction_block(sequence_clean, ID, row_number)\n",
        "end_time = time.time()\n",
        "print(f\"Inference time: {end_time - start_time:.2f}s\")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf_QIjAMaMdp",
        "outputId": "6c2701fc-66dd-47f4-cf31-d9297550a747"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length 1088\n",
            "ptm: 0.441 plddt: 47.891\n",
            "Inference time for entry 8, AGS43817.1_Equine rotavirus A_RNA-dependent RNA polymerase: 202.88s\n",
            "CPU times: user 3min 20s, sys: 3.61 s, total: 3min 24s\n",
            "Wall time: 3min 23s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get inference for files with multiple entries\n",
        "\n",
        "start_time_overall = time.time()\n",
        "\n",
        "for index, row in sequence_file.iterrows():\n",
        "  sequence_clean, meta, copies, ID, u_seqs, length, lengths = sequence_read(\n",
        "      sequence_file,\n",
        "      index,\n",
        "      copies=multimer_n\n",
        "      )\n",
        "\n",
        "  print(f\"Entry {index + 1}: {meta}\")\n",
        "\n",
        "  start_time_loop = time.time()\n",
        "  prediction_block(sequence_clean, ID, row)\n",
        "  end_time_loop = time.time()\n",
        "  print(f\"Inference time for {meta}: \\n{end_time_loop - start_time_loop:.2f}s\")\n",
        "\n",
        "end_time_overall = time.time()\n",
        "print(f\"Overall time: {end_time_overall - start_time_overall:.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niCuUjyBjCJf",
        "outputId": "b755dd64-ae59-4025-d438-3031e8858f8a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length 258\n",
            "Entry 1: YP_009555234.1_Norovirus GII_VP2\n",
            "ptm: 0.279 plddt: 53.239\n",
            "Inference time for YP_009555234.1_Norovirus GII_VP2: \n",
            "6.28s\n",
            "Overall time: 6.28s\n",
            "length 268\n",
            "Entry 2: YP_009518843.1_Norovirus GII_VP2\n",
            "ptm: 0.304 plddt: 53.290\n",
            "Inference time for YP_009518843.1_Norovirus GII_VP2: \n",
            "5.15s\n",
            "Overall time: 11.42s\n",
            "length 211\n",
            "Entry 3: AGX01093.1_Norovirus Hu/GI.2/Jingzhou/2013401/CHN_minor capsid protein VP2\n",
            "ptm: 0.302 plddt: 51.930\n",
            "Inference time for AGX01093.1_Norovirus Hu/GI.2/Jingzhou/2013401/CHN_minor capsid protein VP2: \n",
            "3.11s\n",
            "Overall time: 14.53s\n",
            "length 257\n",
            "Entry 4: AII73775.1_Norovirus GII/Hu/JP/2010/GII.P7_GII.7/Musashimurayama/TAKAsanKimchi_VP2\n",
            "ptm: 0.266 plddt: 50.836\n",
            "Inference time for AII73775.1_Norovirus GII/Hu/JP/2010/GII.P7_GII.7/Musashimurayama/TAKAsanKimchi_VP2: \n",
            "4.80s\n",
            "Overall time: 19.34s\n",
            "length 258\n",
            "Entry 5: CRL46968.1_Norovirus GII/Hu/NL/2014/GII.6/Groningen_capsid protein VP2\n",
            "ptm: 0.258 plddt: 50.443\n",
            "Inference time for CRL46968.1_Norovirus GII/Hu/NL/2014/GII.6/Groningen_capsid protein VP2: \n",
            "4.86s\n",
            "Overall time: 24.20s\n",
            "length 259\n",
            "Entry 6: YP_009518840.1_Norovirus GII.2_VP2\n",
            "ptm: 0.267 plddt: 51.455\n",
            "Inference time for YP_009518840.1_Norovirus GII.2_VP2: \n",
            "4.87s\n",
            "Overall time: 29.07s\n",
            "length 268\n",
            "Entry 7: BAE98192.1_Norovirus Hu/Chiba/04-1050/2005/JP_minor structural protein\n",
            "ptm: 0.300 plddt: 52.910\n",
            "Inference time for BAE98192.1_Norovirus Hu/Chiba/04-1050/2005/JP_minor structural protein: \n",
            "5.16s\n",
            "Overall time: 34.23s\n",
            "length 254\n",
            "Entry 8: ADF47132.1_Norovirus Hu/Shanghai/SH312/2009/CHN_VP2\n",
            "ptm: 0.259 plddt: 51.710\n",
            "Inference time for ADF47132.1_Norovirus Hu/Shanghai/SH312/2009/CHN_VP2: \n",
            "4.23s\n",
            "Overall time: 38.46s\n",
            "length 268\n",
            "Entry 9: AGX01099.1_Norovirus Hu/GII.4/Jingzhou/2013403/CHN_minor capsid protein VP2\n",
            "ptm: 0.303 plddt: 53.799\n",
            "Inference time for AGX01099.1_Norovirus Hu/GII.4/Jingzhou/2013403/CHN_minor capsid protein VP2: \n",
            "5.05s\n",
            "Overall time: 43.51s\n",
            "length 208\n",
            "Entry 10: CRL46953.1_Norovirus GI/Hu/NL/2011/GI.4/Groningen_capsid protein VP2\n",
            "ptm: 0.318 plddt: 54.266\n",
            "Inference time for CRL46953.1_Norovirus GI/Hu/NL/2011/GI.4/Groningen_capsid protein VP2: \n",
            "2.99s\n",
            "Overall time: 46.50s\n",
            "length 259\n",
            "Entry 11: YP_009518837.1_Norovirus GII.17_VP2\n",
            "ptm: 0.251 plddt: 49.335\n",
            "Inference time for YP_009518837.1_Norovirus GII.17_VP2: \n",
            "4.76s\n",
            "Overall time: 51.27s\n",
            "length 208\n",
            "Entry 12: BAB18268.1_Chiba virus_basic protein\n",
            "ptm: 0.322 plddt: 54.672\n",
            "Inference time for BAB18268.1_Chiba virus_basic protein: \n",
            "2.99s\n",
            "Overall time: 54.26s\n",
            "length 261\n",
            "Entry 13: ABC96756.1_Norovirus Hu/OsakaNI/2004/JP_ORF2\n",
            "ptm: 0.240 plddt: 49.059\n",
            "Inference time for ABC96756.1_Norovirus Hu/OsakaNI/2004/JP_ORF2: \n",
            "4.80s\n",
            "Overall time: 59.06s\n",
            "length 264\n",
            "Entry 14: ADC34935.1_Norovirus Hu/GII.14/8533/Maizuru/2008/JPN_minor structural protein\n",
            "ptm: 0.279 plddt: 51.095\n",
            "Inference time for ADC34935.1_Norovirus Hu/GII.14/8533/Maizuru/2008/JPN_minor structural protein: \n",
            "4.93s\n",
            "Overall time: 63.99s\n",
            "length 259\n",
            "Entry 15: ALD83749.1_Norovirus GII/Hu/HKG/2015/GII.Pe_GII.17/CUHK-NS-682_VP2\n",
            "ptm: 0.277 plddt: 53.875\n",
            "Inference time for ALD83749.1_Norovirus GII/Hu/HKG/2015/GII.Pe_GII.17/CUHK-NS-682_VP2: \n",
            "4.77s\n",
            "Overall time: 68.75s\n",
            "length 259\n",
            "Entry 16: BBB86937.1_Norovirus Hu/GII/JP/2014/GII.Pe_GII.2/Saitama-127_capsid protein VP2\n",
            "ptm: 0.263 plddt: 48.489\n",
            "Inference time for BBB86937.1_Norovirus Hu/GII/JP/2014/GII.Pe_GII.2/Saitama-127_capsid protein VP2: \n",
            "4.75s\n",
            "Overall time: 73.51s\n",
            "length 259\n",
            "Entry 17: BBB86973.1_Norovirus Hu/GII/JP/2011/GII.P16_GII.2/Osaka-18_capsid protein VP2\n",
            "ptm: 0.264 plddt: 49.181\n",
            "Inference time for BBB86973.1_Norovirus Hu/GII/JP/2011/GII.P16_GII.2/Osaka-18_capsid protein VP2: \n",
            "4.79s\n",
            "Overall time: 78.30s\n",
            "length 215\n",
            "Entry 18: AZJ17749.1_Norovirus GI.3_VP2\n",
            "ptm: 0.281 plddt: 49.889\n",
            "Inference time for AZJ17749.1_Norovirus GI.3_VP2: \n",
            "3.16s\n",
            "Overall time: 81.46s\n",
            "length 243\n",
            "Entry 19: QDR48007.1_Norovirus GIX_VP2\n",
            "ptm: 0.282 plddt: 51.053\n",
            "Inference time for QDR48007.1_Norovirus GIX_VP2: \n",
            "3.83s\n",
            "Overall time: 85.29s\n",
            "length 215\n",
            "Entry 20: YP_009538342.1_Norovirus GI/Hu/JP/2007/GI.P3_GI.3/Shimizu/KK2866_VP2\n",
            "ptm: 0.278 plddt: 47.455\n",
            "Inference time for YP_009538342.1_Norovirus GI/Hu/JP/2007/GI.P3_GI.3/Shimizu/KK2866_VP2: \n",
            "3.14s\n",
            "Overall time: 88.43s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ptsttm7OpZ96"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}