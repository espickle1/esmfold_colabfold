{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "https://github.com/espickle1/esmfold_colabfold/blob/main/ESMFold_ColabFold_third.ipynb",
      "authorship_tag": "ABX9TyPBAyLNqIfo1EU1BWaDGH2y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/espickle1/esmfold_colabfold/blob/main/ESMFold_ColabFold_third.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpxAvRtxZxrX"
      },
      "outputs": [],
      "source": [
        "### Colabfold running ESMFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Install necessary packages\n",
        "%%time\n",
        "version = \"1\"\n",
        "model_name = \"esmfold_v0.model\" if version == \"0\" else \"esmfold.model\"\n",
        "\n",
        "import os, time\n",
        "\n",
        "if not os.path.isfile(model_name):\n",
        "  # download esmfold params\n",
        "  os.system(\"apt-get install aria2 -qq\")\n",
        "  os.system(f\"aria2c -q -x 16 https://colabfold.steineggerlab.workers.dev/esm/{model_name} &\")\n",
        "\n",
        "  if not os.path.isfile(\"finished_install\"):\n",
        "    # install libs\n",
        "    print(\"installing libs...\")\n",
        "    os.system(\"pip install -q omegaconf pytorch_lightning biopython ml_collections einops py3Dmol modelcif\")\n",
        "    os.system(\"pip install -q git+https://github.com/NVIDIA/dllogger.git\")\n",
        "\n",
        "    print(\"installing openfold...\")\n",
        "    # install openfold\n",
        "    os.system(f\"pip install -q git+https://github.com/sokrypton/openfold.git\")\n",
        "\n",
        "    print(\"installing esmfold...\")\n",
        "    # install esmfold\n",
        "    os.system(f\"pip install -q git+https://github.com/sokrypton/esm.git\")\n",
        "    os.system(\"touch finished_install\")\n",
        "\n",
        "  # wait for Params to finish downloading...\n",
        "  while not os.path.isfile(model_name):\n",
        "    time.sleep(5)\n",
        "  if os.path.isfile(f\"{model_name}.aria2\"):\n",
        "    print(\"downloading params...\")\n",
        "  while os.path.isfile(f\"{model_name}.aria2\"):\n",
        "    time.sleep(5)"
      ],
      "metadata": {
        "id": "jthGQKfGZ1ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import dependencies\n",
        "import torch\n",
        "from jax.tree_util import tree_map\n",
        "import gc\n",
        "\n",
        "from string import ascii_uppercase, ascii_lowercase\n",
        "import hashlib, re, os\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "import py3Dmol\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "A7cyvhSZZ3iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Parse quantitative data for output\n",
        "def parse_output(output):\n",
        "  # Parse PAE and pLDDT\n",
        "  pae = (output[\"aligned_confidence_probs\"][0] * np.arange(64)).mean(-1) * 31\n",
        "  plddt = output[\"plddt\"][0,:,1]\n",
        "\n",
        "  # Parse contacts\n",
        "  bins = np.append(0,np.linspace(2.3125,21.6875,63))\n",
        "  sm_contacts = softmax(output[\"distogram_logits\"],-1)[0]\n",
        "  sm_contacts = sm_contacts[...,bins<8].sum(-1)\n",
        "\n",
        "  # Create overall position matrix\n",
        "  xyz = output[\"positions\"][-1,0,:,1]\n",
        "  mask = output[\"atom37_atom_exists\"][0,:,1] == 1\n",
        "\n",
        "  # Combine output data\n",
        "  o = {\"pae\":pae[mask,:][:,mask],\n",
        "       \"plddt\":plddt[mask],\n",
        "       \"sm_contacts\":sm_contacts[mask,:][:,mask],\n",
        "       \"xyz\":xyz[mask]}\n",
        "\n",
        "  return o\n",
        "\n",
        "## Get has for unique item ID\n",
        "def get_hash(x): return hashlib.sha1(x.encode()).hexdigest()"
      ],
      "metadata": {
        "id": "cJtnYl8SZ7px"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load model\n",
        "if \"model\" not in dir() or model_name != model_name_:\n",
        "  if \"model\" in dir():\n",
        "    # delete old model from memory\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  # Load the model directly\n",
        "  model = torch.load(model_name)\n",
        "  model.eval().cuda().requires_grad_(False)\n",
        "  model_name_ = model_name"
      ],
      "metadata": {
        "id": "DPsKQP0AaCAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import as a function\n",
        "def sequence_read(sequence_input, position, copies, start_position):\n",
        "  # Read translated sequences and parse to clean up invalid charaacters\n",
        "  sequence = sequence_input.loc[position]['Translation']\n",
        "  sequence_clean = re.sub(\"[^A-Z:]\", \"\", sequence.replace(\"/\",\":\").upper())\n",
        "  sequence_clean = re.sub(\":+\",\":\",sequence)\n",
        "  sequence_clean = re.sub(\"^[:]+\",\"\",sequence)\n",
        "  sequence_clean = re.sub(\"[:]+$\",\"\",sequence)\n",
        "\n",
        "  # Read meta data from the file\n",
        "  meta = sequence_input.loc[position]['meta']\n",
        "\n",
        "  # Join multimers into one sequence\n",
        "  if copies == \"\" or copies <= 0: copies = 1\n",
        "  sequence = \":\".join([sequence] * copies)\n",
        "\n",
        "  # Come up with sequence ID and overall predicted length\n",
        "  position_adjusted = position + start_position\n",
        "  ID = \"No_\" + str(position_adjusted) + \"_\" + jobname+\"_\"+get_hash(sequence)[:5]\n",
        "  seqs = sequence.split(\":\")\n",
        "  lengths = [len(s) for s in seqs]\n",
        "  length = sum(lengths)\n",
        "\n",
        "  # Determine the type of multimer\n",
        "  u_seqs = list(set(seqs))\n",
        "  if len(seqs) == 1: mode = \"mono\"\n",
        "  elif len(u_seqs) == 1: mode = \"homo\"\n",
        "  else: mode = \"hetero\"\n",
        "\n",
        "  return sequence_clean, meta, copies, ID, u_seqs, length, lengths"
      ],
      "metadata": {
        "id": "F-o5Z2O7aDuw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Predict protein structure from input sequence\n",
        "def prediction_block(sequence, ID, row_number):\n",
        "  # Determine chunk size for speedup\n",
        "  # Following scheme is used (700 cutoff)\n",
        "  # A100: 312 for smaller, 192 for larger\n",
        "  # L4: 256 for smaller, 164 for larger\n",
        "  # T4: 128 for smaller, 64 for larger\n",
        "  length = len(sequence)\n",
        "  if length > 700:\n",
        "    model.set_chunk_size(192)\n",
        "  else:\n",
        "    model.set_chunk_size(312)\n",
        "\n",
        "  # Empty cuda cache\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Inference\n",
        "  output = model.infer(\n",
        "      sequence,\n",
        "      num_recycles=num_recycles,\n",
        "      chain_linker=\"X\"*chain_linker,\n",
        "      residue_index_offset=512\n",
        "      )\n",
        "\n",
        "  # Parse outputs into exportable format\n",
        "  pdb_str = model.output_to_pdb(output)[0]\n",
        "  output = tree_map(lambda x: x.cpu().numpy(), output)\n",
        "  ptm = output[\"ptm\"][0]\n",
        "  plddt = output[\"plddt\"][0,...,1].mean()\n",
        "  O = parse_output(output)\n",
        "  print(f'ptm: {ptm:.3f} plddt: {plddt:.3f}')\n",
        "\n",
        "  # Save results into files\n",
        "  prefix = f\"{ID}_ptm{ptm:.3f}_r{num_recycles}_default\"\n",
        "  np.savetxt(f\"{prefix}.pae.txt\",O[\"pae\"],\"%.3f\")\n",
        "  with open(f\"{prefix}.pdb\",\"w\") as out:\n",
        "    out.write(pdb_str)\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "YcThVBCJaFHh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import settings: manual settings\n",
        "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
        "num_recycles = 3\n",
        "chain_linker = 25\n",
        "multimer_n = 1\n",
        "\n",
        "# Job names for batches\n",
        "jobname = \"dir_test\"\n",
        "jobname = re.sub(r'\\W+', '', jobname)[:50]\n",
        "\n",
        "# File paths and names\n",
        "input_directory = \"/content/drive/MyDrive/ww_virome/esmfold_colab/sequences/\"\n",
        "output_directory = \"/content/drive/MyDrive/ww_virome/esmfold_colab/structures\"\n",
        "os.chdir(output_directory)\n",
        "file_path = f\"{input_directory}translations_1201_1300.csv\"\n",
        "sequence_file = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "nQZ7NeD4aG6Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Get inference for files with multiple entries\n",
        "# Start the timer\n",
        "start_position = 601\n",
        "start_time_overall = time.time()\n",
        "\n",
        "# Start the loop to iterate down the list\n",
        "for index, row in sequence_file.iterrows():\n",
        "  # Read sequence for the row being processed\n",
        "  sequence_clean, meta, copies, ID, u_seqs, length, lengths = sequence_read(\n",
        "      sequence_file,\n",
        "      index,\n",
        "      copies=multimer_n,\n",
        "      start_position=start_position\n",
        "      )\n",
        "  print(f\"-----\")\n",
        "  print(f\"Entry {index + 1 + start_position}: {meta}\")\n",
        "  print(f\"Sequence length: {length}\")\n",
        "\n",
        "  # Infer the structure\n",
        "  start_time_loop = time.time()\n",
        "  prediction_block(sequence_clean, ID, row)\n",
        "  end_time_loop = time.time()\n",
        "  print(f\"Inference time for {meta}: \\n{end_time_loop - start_time_loop:.2f}s \\n\")\n",
        "\n",
        "# End the timer\n",
        "end_time_overall = time.time()\n",
        "print(f\"Overall time: {end_time_overall - start_time_overall:.2f}s\")"
      ],
      "metadata": {
        "id": "niCuUjyBjCJf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}